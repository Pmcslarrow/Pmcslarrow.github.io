<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>High-Performance Image Processing via Parallel Contrast Stretching | McSlarrowPortfolio</title><meta name=keywords content="c++,mpi,hpc,parallel"><meta name=description content="Built with MPI in C++"><meta name=author content="Me"><link rel=canonical href=http://localhost:1313/projects/1_contrast_stretching/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/projects/1_contrast_stretching/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/projects/1_contrast_stretching/"><meta property="og:site_name" content="McSlarrowPortfolio"><meta property="og:title" content="High-Performance Image Processing via Parallel Contrast Stretching"><meta property="og:description" content="Built with MPI in C++"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2024-05-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-05-01T00:00:00+00:00"><meta property="article:tag" content="C++"><meta property="article:tag" content="Mpi"><meta property="article:tag" content="Hpc"><meta property="article:tag" content="Parallel"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="High-Performance Image Processing via Parallel Contrast Stretching"><meta name=twitter:description content="Built with MPI in C++"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"http://localhost:1313/projects/"},{"@type":"ListItem","position":2,"name":"High-Performance Image Processing via Parallel Contrast Stretching","item":"http://localhost:1313/projects/1_contrast_stretching/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"High-Performance Image Processing via Parallel Contrast Stretching","name":"High-Performance Image Processing via Parallel Contrast Stretching","description":"Built with MPI in C++","keywords":["c++","mpi","hpc","parallel"],"articleBody":"Note to reader I do not include large code snippets as this was an assignment for my parallel computing course at Northwestern and I don’t want future students copying it. It exists in a private repo of mine containing my school assignments. I will try to do my best to illustrate the work I did with fewer code examples\nGithub Link Private url Overview This project parallelizes a contrast stretching algorithm using MPI to enable high-performance image processing on distributed systems. The final implementation achieved approximately a 10× speedup over the sequential baseline.\nTo get there, I first created a shared-memory version using OpenMP. While this brought notable gains, the most interesting and scalable results came from the MPI-based distributed implementation. Special considerations—like ghost rows, data convergence, and uneven chunk sizes—were handled carefully to ensure both correctness and performance.\nWhat Is Contrast Stretching? Contrast stretching enhances image clarity by expanding the range of pixel intensity values.\nIt’s essentially a nearest neighbors operation, where each pixel’s new value depends on its neighboring pixels. In a sequential setting, this simply means iterating over every non-boundary pixel and adjusting its value accordingly.\nThe Sequential vs. Distributed Approach In a naïve distributed version, each worker could receive the entire image, process its part, and return results. But that would be extremely memory-inefficient.\nInstead, a more optimal strategy involves:\nSplitting the image into horizontal chunks (rows). Distributing chunks among workers. Each worker processes only its assigned chunk. Processed chunks are gathered and stitched together. // // When it is the main process, we are scattering the image. // uchar* sendbuf = (rank == 0) ? image[0] : NULL; // // New2dMatrix flattens a matrix into a 1D representation // This is the recieve buffer. We let the main process handle leftover rows // uchar** subset_buf = (rank == 0) ? New2dMatrix\u003cuchar\u003e(leftover_chunk_size + 2, cols * 3) : New2dMatrix\u003cuchar\u003e(chunk_size + 2, cols * 3); // // Scattering to each worker based on the size (counts) and offsets (displacements) // MPI_Scatterv( sendbuf, counts, displacements, MPI_UNSIGNED_CHAR, subset_buf[1], counts[rank], MPI_UNSIGNED_CHAR, 0, MPI_COMM_WORLD ); This high-level strategy solves the memory issue, but introduces three key technical challenges.\nChallenge 1: Ghost Rows (Halo Exchange) Since contrast stretching depends on neighboring pixels, each worker also needs access to the rows immediately above and below its chunk—even though they belong to another worker.\nExample:\nWorker 1 needs: The top row from Worker 2. The bottom row from Worker 0. These are known as ghost rows and must be exchanged between adjacent processes before each computation step.\n// // Changing the destination and src based on the current node rank // int dest = (rank \u003c numProcs - 1) ? rank + 1 : MPI_PROC_NULL; int src = (rank \u003e 0) ? rank - 1 : MPI_PROC_NULL; // // Halo exchanges // MPI_Sendrecv( image[rows], COLS, MPI_UNSIGNED_CHAR, dest, tag, image[0], COLS, MPI_UNSIGNED_CHAR, src, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE ); MPI_Sendrecv( image[1], COLS, MPI_UNSIGNED_CHAR, src, tag, image[rows+1], COLS, MPI_UNSIGNED_CHAR, dest, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE ); Challenge 2: Alignment and Convergence In a distributed setting, it’s critical that all processes are working with consistent and up-to-date data. Misaligned ghost rows can cause incorrect results.\nTo handle this:\nAfter each processing round, workers exchange ghost rows with neighbors. This ensures alignment across all parts of the image. Convergence is achieved when no further changes occur across any worker’s chunk. int step = 1; bool converged = false; while (step \u003c= steps \u0026\u0026 !converged) { // Ghost exchange // Each worker processes their chunk here // ... // // Gathering the number of pixel differences from each worker // int global_diffs = 0; MPI_Reduce(\u0026diffs, \u0026global_diffs, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD); MPI_Bcast(\u0026global_diffs, 1, MPI_INT, 0 /* sender */, MPI_COMM_WORLD); // // If the number of differences across all workers is 0, then we can stop going through the loop // converged = (global_diffs == 0); } Challenge 3: Uneven Chunk Sizes Image dimensions don’t always divide evenly across the number of processes. If the height of the image isn’t divisible by the number of workers, chunk sizes will vary.\nTo address this:\nMPI_Scatterv is used to distribute varying chunk sizes. Each worker handles the correct number of rows, and the final image is reconstructed seamlessly. // // This contains the chunk sizes for each worker: // int counts = { 308, 307, 307, 307, 307 } for n = 5 // int counts[numProcs]; counts[0] = leftover_chunk_size; for (int i = 1; i \u003c numProcs; i++) { counts[i] = chunk_size; } // // This calculates the starting position for each process's chunk // displacements = {0, 308, 615, 922, 1229}; // int displacements[numProcs]; displacements[0] = 0; for (int i = 1; i \u003c numProcs; i++) { displacements[i] = displacements[i - 1] + counts[i - 1]; } // // Scaling based on the three color channels (RGB) // for (int i = 0; i \u003c numProcs; i++) { counts[i] *= cols * 3; displacements[i] *= cols * 3; } Speedup on my local macbook MPI Distributed Solution (Steps = 75, Nodes = 7) ** Done! Time: 8.855 secs ** Writing bitmap... ** Execution complete. Sequential Solution (Steps = 75) ** Done! Time: 46.147 secs ** Writing bitmap... ** Execution complete. Local Speedup = 5.21x Northwestern’s HPC, Quest, achieved ~10x speedup Summary Through careful parallelization of a nearest-neighbor image algorithm, this project demonstrates a scalable and efficient approach to contrast stretching using MPI.\nKey outcomes:\n~10× speedup over the sequential version. Handled distributed data dependencies with ghost rows. Used Scatterv to handle uneven workload distribution. Achieved convergence with synchronized communication between processes. Utilized Northwestern’s high performance computing cluster, Quest, to achieve even better results. ","wordCount":"938","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-05-01T00:00:00Z","dateModified":"2024-05-01T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/projects/1_contrast_stretching/"},"publisher":{"@type":"Organization","name":"McSlarrowPortfolio","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title="about me"><span>about me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/projects/>Projects</a></div><h1 class="post-title entry-hint-parent">High-Performance Image Processing via Parallel Contrast Stretching</h1><div class=post-description>Built with MPI in C++</div></header><div class=post-content><h2 id=note-to-reader>Note to reader<a hidden class=anchor aria-hidden=true href=#note-to-reader>#</a></h2><p><em>I do not include large code snippets as this was an assignment for my parallel computing course at Northwestern and I don&rsquo;t want future students copying it. It exists in a private repo of mine containing my school assignments. I will try to do my best to illustrate the work I did with fewer code examples</em></p><h2 id=github-link>Github Link<a hidden class=anchor aria-hidden=true href=#github-link>#</a></h2><ul><li>Private url</li></ul><h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><p>This project parallelizes a contrast stretching algorithm using MPI to enable high-performance image processing on distributed systems. The final implementation achieved approximately a <strong>10× speedup</strong> over the sequential baseline.</p><p>To get there, I first created a shared-memory version using OpenMP. While this brought notable gains, the most interesting and scalable results came from the MPI-based distributed implementation. Special considerations—like <strong>ghost rows</strong>, <strong>data convergence</strong>, and <strong>uneven chunk sizes</strong>—were handled carefully to ensure both correctness and performance.</p><h2 id=what-is-contrast-stretching>What Is Contrast Stretching?<a hidden class=anchor aria-hidden=true href=#what-is-contrast-stretching>#</a></h2><p>Contrast stretching enhances image clarity by expanding the range of pixel intensity values.</p><p>It&rsquo;s essentially a <em>nearest neighbors</em> operation, where each pixel’s new value depends on its neighboring pixels. In a sequential setting, this simply means iterating over every non-boundary pixel and adjusting its value accordingly.</p><div><img src=/images/contrast_stretching/contrast.png alt="Example of contrast stretching" width=500px style="display:block;margin:0 auto"></div><h2 id=the-sequential-vs-distributed-approach>The Sequential vs. Distributed Approach<a hidden class=anchor aria-hidden=true href=#the-sequential-vs-distributed-approach>#</a></h2><p>In a naïve distributed version, each worker could receive the entire image, process its part, and return results. But that would be <strong>extremely memory-inefficient</strong>.</p><p>Instead, a more optimal strategy involves:</p><ul><li>Splitting the image into horizontal chunks (rows).</li><li>Distributing chunks among workers.</li><li>Each worker processes only its assigned chunk.</li><li>Processed chunks are gathered and stitched together.</li></ul><div><img src=/images/contrast_stretching/scattering.png alt="Scattering an image across workers" width=500px style="display:block;margin:0 auto"></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1>// When it is the main process, we are scattering the image. 
</span></span></span><span class=line><span class=cl><span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>uchar</span><span class=o>*</span> <span class=n>sendbuf</span> <span class=o>=</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=o>?</span> <span class=n>image</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>:</span> <span class=nb>NULL</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1>// New2dMatrix flattens a matrix into a 1D representation
</span></span></span><span class=line><span class=cl><span class=c1>// This is the recieve buffer. We let the main process handle leftover rows
</span></span></span><span class=line><span class=cl><span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>uchar</span><span class=o>**</span> <span class=n>subset_buf</span> <span class=o>=</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=o>?</span> <span class=n>New2dMatrix</span><span class=o>&lt;</span><span class=n>uchar</span><span class=o>&gt;</span><span class=p>(</span><span class=n>leftover_chunk_size</span> <span class=o>+</span> <span class=mi>2</span><span class=p>,</span> <span class=n>cols</span> <span class=o>*</span> <span class=mi>3</span><span class=p>)</span> <span class=o>:</span> <span class=n>New2dMatrix</span><span class=o>&lt;</span><span class=n>uchar</span><span class=o>&gt;</span><span class=p>(</span><span class=n>chunk_size</span> <span class=o>+</span> <span class=mi>2</span><span class=p>,</span> <span class=n>cols</span> <span class=o>*</span> <span class=mi>3</span><span class=p>);</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1>// Scattering to each worker based on the size (counts) and offsets (displacements)
</span></span></span><span class=line><span class=cl><span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>MPI_Scatterv</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>sendbuf</span><span class=p>,</span> <span class=n>counts</span><span class=p>,</span> <span class=n>displacements</span><span class=p>,</span> <span class=n>MPI_UNSIGNED_CHAR</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>subset_buf</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>counts</span><span class=p>[</span><span class=n>rank</span><span class=p>],</span> <span class=n>MPI_UNSIGNED_CHAR</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span>
</span></span><span class=line><span class=cl><span class=p>);</span>
</span></span></code></pre></div><p>This high-level strategy solves the memory issue, but introduces <strong>three key technical challenges</strong>.</p><hr><h2 id=challenge-1-ghost-rows-halo-exchange>Challenge 1: Ghost Rows (Halo Exchange)<a hidden class=anchor aria-hidden=true href=#challenge-1-ghost-rows-halo-exchange>#</a></h2><p>Since contrast stretching depends on neighboring pixels, each worker also needs access to the rows immediately above and below its chunk—<em>even though they belong to another worker</em>.</p><p>Example:</p><ul><li>Worker 1 needs:<ul><li>The <strong>top row</strong> from Worker 2.</li><li>The <strong>bottom row</strong> from Worker 0.</li></ul></li></ul><p>These are known as <strong>ghost rows</strong> and must be exchanged between adjacent processes before each computation step.</p><div><img src=/images/contrast_stretching/ghost_rows.png alt="Ghost rows image" width=250px style="display:block;margin:0 auto"></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1>// Changing the destination and src based on the current node rank
</span></span></span><span class=line><span class=cl><span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>int</span> <span class=n>dest</span> <span class=o>=</span> <span class=p>(</span><span class=n>rank</span> <span class=o>&lt;</span> <span class=n>numProcs</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>?</span> <span class=n>rank</span> <span class=o>+</span> <span class=mi>1</span> <span class=o>:</span> <span class=n>MPI_PROC_NULL</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>src</span> <span class=o>=</span> <span class=p>(</span><span class=n>rank</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>)</span> <span class=o>?</span> <span class=n>rank</span> <span class=o>-</span> <span class=mi>1</span> <span class=o>:</span> <span class=n>MPI_PROC_NULL</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1>// Halo exchanges
</span></span></span><span class=line><span class=cl><span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>MPI_Sendrecv</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>image</span><span class=p>[</span><span class=n>rows</span><span class=p>],</span> <span class=n>COLS</span><span class=p>,</span> <span class=n>MPI_UNSIGNED_CHAR</span><span class=p>,</span> <span class=n>dest</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>image</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>COLS</span><span class=p>,</span> <span class=n>MPI_UNSIGNED_CHAR</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_STATUS_IGNORE</span>
</span></span><span class=line><span class=cl><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>MPI_Sendrecv</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>image</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>COLS</span><span class=p>,</span> <span class=n>MPI_UNSIGNED_CHAR</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>image</span><span class=p>[</span><span class=n>rows</span><span class=o>+</span><span class=mi>1</span><span class=p>],</span> <span class=n>COLS</span><span class=p>,</span> <span class=n>MPI_UNSIGNED_CHAR</span><span class=p>,</span> <span class=n>dest</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_STATUS_IGNORE</span>
</span></span><span class=line><span class=cl><span class=p>);</span>
</span></span></code></pre></div><hr><h2 id=challenge-2-alignment-and-convergence>Challenge 2: Alignment and Convergence<a hidden class=anchor aria-hidden=true href=#challenge-2-alignment-and-convergence>#</a></h2><p>In a distributed setting, it&rsquo;s critical that all processes are working with <strong>consistent and up-to-date data</strong>. Misaligned ghost rows can cause incorrect results.</p><p>To handle this:</p><ul><li>After each processing round, workers exchange ghost rows with neighbors.</li><li>This ensures alignment across all parts of the image.</li><li><strong>Convergence</strong> is achieved when no further changes occur across any worker&rsquo;s chunk.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>step</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=kt>bool</span> <span class=n>converged</span> <span class=o>=</span> <span class=nb>false</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>while</span> <span class=p>(</span><span class=n>step</span> <span class=o>&lt;=</span> <span class=n>steps</span> <span class=o>&amp;&amp;</span> <span class=o>!</span><span class=n>converged</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Ghost exchange
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// Each worker processes their chunk here
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// ...
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>    <span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// Gathering the number of pixel differences from each worker
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>int</span> <span class=n>global_diffs</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Reduce</span><span class=p>(</span><span class=o>&amp;</span><span class=n>diffs</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>global_diffs</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=n>MPI_SUM</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>MPI_Bcast</span><span class=p>(</span><span class=o>&amp;</span><span class=n>global_diffs</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>MPI_INT</span><span class=p>,</span> <span class=mi>0</span> <span class=cm>/* sender */</span><span class=p>,</span> <span class=n>MPI_COMM_WORLD</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// If the number of differences across all workers is 0, then we can stop going through the loop
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>converged</span> <span class=o>=</span> <span class=p>(</span><span class=n>global_diffs</span> <span class=o>==</span> <span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><hr><h2 id=challenge-3-uneven-chunk-sizes>Challenge 3: Uneven Chunk Sizes<a hidden class=anchor aria-hidden=true href=#challenge-3-uneven-chunk-sizes>#</a></h2><p>Image dimensions don&rsquo;t always divide evenly across the number of processes. If the height of the image isn’t divisible by the number of workers, chunk sizes will vary.</p><p>To address this:</p><ul><li><strong><code>MPI_Scatterv</code></strong> is used to distribute varying chunk sizes.</li><li>Each worker handles the correct number of rows, and the final image is reconstructed seamlessly.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// 
</span></span></span><span class=line><span class=cl><span class=c1>// This contains the chunk sizes for each worker:
</span></span></span><span class=line><span class=cl><span class=c1>// int counts = { 308, 307, 307, 307, 307 } for n = 5 
</span></span></span><span class=line><span class=cl><span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>int</span> <span class=n>counts</span><span class=p>[</span><span class=n>numProcs</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=n>counts</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>leftover_chunk_size</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>numProcs</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>counts</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>chunk_size</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1>// This calculates the starting position for each process&#39;s chunk
</span></span></span><span class=line><span class=cl><span class=c1>// displacements = {0, 308, 615, 922, 1229};
</span></span></span><span class=line><span class=cl><span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>int</span> <span class=n>displacements</span><span class=p>[</span><span class=n>numProcs</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=n>displacements</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>numProcs</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>displacements</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>displacements</span><span class=p>[</span><span class=n>i</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>counts</span><span class=p>[</span><span class=n>i</span> <span class=o>-</span> <span class=mi>1</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1>// Scaling based on the three color channels (RGB)
</span></span></span><span class=line><span class=cl><span class=c1>//
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>numProcs</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>counts</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>*=</span> <span class=n>cols</span> <span class=o>*</span> <span class=mi>3</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>displacements</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>*=</span> <span class=n>cols</span> <span class=o>*</span> <span class=mi>3</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h2 id=speedup-on-my-local-macbook>Speedup on my local macbook<a hidden class=anchor aria-hidden=true href=#speedup-on-my-local-macbook>#</a></h2><h4 id=mpi-distributed-solution-steps--75-nodes--7>MPI Distributed Solution (Steps = 75, Nodes = 7)<a hidden class=anchor aria-hidden=true href=#mpi-distributed-solution-steps--75-nodes--7>#</a></h4><pre tabindex=0><code>    ** Done!  Time: 8.855 secs
    ** Writing bitmap...
    ** Execution complete.
</code></pre><h4 id=sequential-solution-steps--75>Sequential Solution (Steps = 75)<a hidden class=anchor aria-hidden=true href=#sequential-solution-steps--75>#</a></h4><pre tabindex=0><code>    ** Done!  Time: 46.147 secs
    ** Writing bitmap...
    ** Execution complete.
</code></pre><h4 id=local-speedup--521x>Local Speedup = 5.21x<a hidden class=anchor aria-hidden=true href=#local-speedup--521x>#</a></h4><h4 id=northwesterns-hpc-quest-achieved-10x-speedup>Northwestern&rsquo;s HPC, Quest, achieved ~10x speedup<a hidden class=anchor aria-hidden=true href=#northwesterns-hpc-quest-achieved-10x-speedup>#</a></h4><hr><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>Through careful parallelization of a nearest-neighbor image algorithm, this project demonstrates a scalable and efficient approach to contrast stretching using MPI.</p><p>Key outcomes:</p><ul><li>~10× speedup over the sequential version.</li><li>Handled distributed data dependencies with ghost rows.</li><li>Used <code>Scatterv</code> to handle uneven workload distribution.</li><li>Achieved convergence with synchronized communication between processes.</li><li>Utilized Northwestern&rsquo;s high performance computing cluster, Quest, to achieve even better results.</li></ul><hr></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/c++/>C++</a></li><li><a href=http://localhost:1313/tags/mpi/>Mpi</a></li><li><a href=http://localhost:1313/tags/hpc/>Hpc</a></li><li><a href=http://localhost:1313/tags/parallel/>Parallel</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>McSlarrowPortfolio</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>