<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DL HW4 (RNN/LSTMs) | McSlarrowPortfolio</title><meta name=keywords content="ML,DL,Python,PyTorch"><meta name=description content="Note to reader
I do not include large code snippets as this was an assignment for my deep learning course at Northwestern and I don&rsquo;t want future students copying it. It exists in a private repo of mine containing my school assignments. I will try to do my best to illustrate the work I did with fewer code examples
Github Link

Private url

Overview
This assignment challenged us to learn more about sequence modeling, and better understand the conceptual pros/cons between RNNs and LSTMs. We implemented RNNs, LSTMs, and a deep reinforcement learning model to play pong. I find that the sequence modeling and autoencoder information is more interesting in this project, so I didn&rsquo;t include it here."><meta name=author content="Me"><link rel=canonical href=http://localhost:1313/projects/4_dl_hw4/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/projects/4_dl_hw4/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/projects/4_dl_hw4/"><meta property="og:site_name" content="McSlarrowPortfolio"><meta property="og:title" content="DL HW4 (RNN/LSTMs)"><meta property="og:description" content="Note to reader I do not include large code snippets as this was an assignment for my deep learning course at Northwestern and I don’t want future students copying it. It exists in a private repo of mine containing my school assignments. I will try to do my best to illustrate the work I did with fewer code examples
Github Link Private url Overview This assignment challenged us to learn more about sequence modeling, and better understand the conceptual pros/cons between RNNs and LSTMs. We implemented RNNs, LSTMs, and a deep reinforcement learning model to play pong. I find that the sequence modeling and autoencoder information is more interesting in this project, so I didn’t include it here."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2024-05-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-05-01T00:00:00+00:00"><meta property="article:tag" content="ML"><meta property="article:tag" content="DL"><meta property="article:tag" content="Python"><meta property="article:tag" content="PyTorch"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="DL HW4 (RNN/LSTMs)"><meta name=twitter:description content="Note to reader
I do not include large code snippets as this was an assignment for my deep learning course at Northwestern and I don&rsquo;t want future students copying it. It exists in a private repo of mine containing my school assignments. I will try to do my best to illustrate the work I did with fewer code examples
Github Link

Private url

Overview
This assignment challenged us to learn more about sequence modeling, and better understand the conceptual pros/cons between RNNs and LSTMs. We implemented RNNs, LSTMs, and a deep reinforcement learning model to play pong. I find that the sequence modeling and autoencoder information is more interesting in this project, so I didn&rsquo;t include it here."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"http://localhost:1313/projects/"},{"@type":"ListItem","position":2,"name":"DL HW4 (RNN/LSTMs)","item":"http://localhost:1313/projects/4_dl_hw4/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"DL HW4 (RNN/LSTMs)","name":"DL HW4 (RNN\/LSTMs)","description":"Note to reader I do not include large code snippets as this was an assignment for my deep learning course at Northwestern and I don\u0026rsquo;t want future students copying it. It exists in a private repo of mine containing my school assignments. I will try to do my best to illustrate the work I did with fewer code examples\nGithub Link Private url Overview This assignment challenged us to learn more about sequence modeling, and better understand the conceptual pros/cons between RNNs and LSTMs. We implemented RNNs, LSTMs, and a deep reinforcement learning model to play pong. I find that the sequence modeling and autoencoder information is more interesting in this project, so I didn\u0026rsquo;t include it here.\n","keywords":["ML","DL","Python","PyTorch"],"articleBody":"Note to reader I do not include large code snippets as this was an assignment for my deep learning course at Northwestern and I don’t want future students copying it. It exists in a private repo of mine containing my school assignments. I will try to do my best to illustrate the work I did with fewer code examples\nGithub Link Private url Overview This assignment challenged us to learn more about sequence modeling, and better understand the conceptual pros/cons between RNNs and LSTMs. We implemented RNNs, LSTMs, and a deep reinforcement learning model to play pong. I find that the sequence modeling and autoencoder information is more interesting in this project, so I didn’t include it here.\nPart 1: Masked Autoencoder What is it: Masked autoencoding zeros out parts of an image, and trains the autoencoder to reconstruct the original image from partially masked inputs. The task: We were tasked with training a simple multi-layer perceptron model to act as an autoencoder, to reconstruct corrupted input into the correct input. Findings: The MLP fails to effectively reconstruct the input image. It fails to do so because the model was trained to reconstruct a masked left half of an image, therefore, the model doesn’t fully understand that how to differentiate a 3 from an 8. This explains why the left half is less prominent – there were times during training where it correctly learned to associate a 3 with a 3, but there were times where the right half of the image looked like a 3, but happened to be an 8. Therefore our reconstruction, while wrong, and an incredibly interesting concept to think about. It is also a great reminder that models will only know to predict things within its learned weights and biases, and cannot go outside of this distribution. Part 2: Sequence learning with a LSTM and RNN Theoretically, why would we expect an LSTM to outperform a standard RNN on problems with long sequences? In a traditional RNN, when weights are less than 1, as the length of the sequence increases, backpropogation would have the weights approach 0, and when the weights are greater than 1, they approach infinity. An LSTM helps mitigate against the exploding/vanishing gradients problem because they use gating mechanisms to keep gradients stable by maintaing and forgetting long term information over time. We example this theory with an analysis from my PyTorch solution RNN Training Loss LSTM Loss Conclusion This example helps support the theory above, that the LSTM helps mitigate against the exploding/vanishing gradients problem as the sequence length increases. ","wordCount":"429","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-05-01T00:00:00Z","dateModified":"2024-05-01T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/projects/4_dl_hw4/"},"publisher":{"@type":"Organization","name":"McSlarrowPortfolio","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title="about me"><span>about me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/projects/>Projects</a></div><h1 class="post-title entry-hint-parent">DL HW4 (RNN/LSTMs)</h1></header><div class=post-content><h2 id=note-to-reader>Note to reader<a hidden class=anchor aria-hidden=true href=#note-to-reader>#</a></h2><p><em>I do not include large code snippets as this was an assignment for my deep learning course at Northwestern and I don&rsquo;t want future students copying it. It exists in a private repo of mine containing my school assignments. I will try to do my best to illustrate the work I did with fewer code examples</em></p><h2 id=github-link>Github Link<a hidden class=anchor aria-hidden=true href=#github-link>#</a></h2><ul><li>Private url</li></ul><h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><p>This assignment challenged us to learn more about sequence modeling, and better understand the conceptual pros/cons between RNNs and LSTMs. We implemented RNNs, LSTMs, and a deep reinforcement learning model to play pong. I find that the sequence modeling and autoencoder information is more interesting in this project, so I didn&rsquo;t include it here.</p><h2 id=part-1-masked-autoencoder>Part 1: Masked Autoencoder<a hidden class=anchor aria-hidden=true href=#part-1-masked-autoencoder>#</a></h2><h4 id=what-is-it>What is it:<a hidden class=anchor aria-hidden=true href=#what-is-it>#</a></h4><ul><li>Masked autoencoding zeros out parts of an image, and trains the autoencoder to reconstruct the original image from partially masked inputs.</li></ul><h4 id=the-task>The task:<a hidden class=anchor aria-hidden=true href=#the-task>#</a></h4><ul><li>We were tasked with training a simple multi-layer perceptron model to act as an autoencoder, to reconstruct corrupted input into the correct input.</li></ul><h4 id=findings>Findings:<a hidden class=anchor aria-hidden=true href=#findings>#</a></h4><ul><li>The MLP fails to effectively reconstruct the input image. It fails to do so because the model was trained to reconstruct a masked left half of an image, therefore, the model doesn&rsquo;t fully understand that how to differentiate a 3 from an 8. This explains why the left half is less prominent &ndash; there were times during training where it correctly learned to associate a 3 with a 3, but there were times where the right half of the image looked like a 3, but happened to be an 8. Therefore our reconstruction, while wrong, and an incredibly interesting concept to think about.</li><li>It is also a great reminder that models will only know to predict things within its learned weights and biases, and cannot go outside of this distribution.</li></ul><div><img src=/images/dl_hw4/masked_autoencoder.png alt="Ghost rows image" width=500px style="display:block;margin:0 auto"></div><hr><h2 id=part-2-sequence-learning-with-a-lstm-and-rnn>Part 2: Sequence learning with a LSTM and RNN<a hidden class=anchor aria-hidden=true href=#part-2-sequence-learning-with-a-lstm-and-rnn>#</a></h2><h4 id=theoretically-why-would-we-expect-an-lstm-to-outperform-a-standard-rnn-on-problems-with-long-sequences>Theoretically, why would we expect an LSTM to outperform a standard RNN on problems with long sequences?<a hidden class=anchor aria-hidden=true href=#theoretically-why-would-we-expect-an-lstm-to-outperform-a-standard-rnn-on-problems-with-long-sequences>#</a></h4><ul><li>In a traditional RNN, when weights are less than 1, as the length of the sequence increases, backpropogation would have the weights approach 0, and when the weights are greater than 1, they approach infinity. An LSTM helps mitigate against the exploding/vanishing gradients problem because they use gating mechanisms to keep gradients stable by maintaing and forgetting long term information over time.</li><li>We example this theory with an analysis from my PyTorch solution</li></ul><h4 id=rnn-training-loss>RNN Training Loss<a hidden class=anchor aria-hidden=true href=#rnn-training-loss>#</a></h4><div><img src=/images/dl_hw4/rnn_loss.png alt="RNN Loss" width=500px style="display:block;margin:0 auto"></div><h4 id=lstm-loss>LSTM Loss<a hidden class=anchor aria-hidden=true href=#lstm-loss>#</a></h4><div><img src=/images/dl_hw4/lstm_loss.png alt="LSTM Loss" width=500px style="display:block;margin:0 auto"></div><h4 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h4><ul><li>This example helps support the theory above, that the LSTM helps mitigate against the exploding/vanishing gradients problem as the sequence length increases.</li></ul><hr></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/ml/>ML</a></li><li><a href=http://localhost:1313/tags/dl/>DL</a></li><li><a href=http://localhost:1313/tags/python/>Python</a></li><li><a href=http://localhost:1313/tags/pytorch/>PyTorch</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>McSlarrowPortfolio</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>