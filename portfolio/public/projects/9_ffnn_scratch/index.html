<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Feed-forward Neural Network from Scratch | ExampleSite</title><meta name=keywords content="Python,PyTorch"><meta name=description content="Note to reader
This was an assignment for my ML class at Northwestern, and while I do include the code in this, I want to be clear that this is my group&rsquo;s work and should not be used for any future assignments.
Overview
This assignment was a stepping stone in my interest in machine learning. We were given multiple datasets to work with based on a few different distributions that I would have to learn from using a neural network. We first had to implement a PyTorch simple neural network to solve the problem(s), and once we understood what the expected solutions should look similar to, we had to implement our solutions completely from scratch."><meta name=author content="Me"><link rel=canonical href=http://localhost:1313/projects/9_ffnn_scratch/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/projects/9_ffnn_scratch/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/projects/9_ffnn_scratch/"><meta property="og:site_name" content="ExampleSite"><meta property="og:title" content="Feed-forward Neural Network from Scratch"><meta property="og:description" content="Note to reader This was an assignment for my ML class at Northwestern, and while I do include the code in this, I want to be clear that this is my group’s work and should not be used for any future assignments.
Overview This assignment was a stepping stone in my interest in machine learning. We were given multiple datasets to work with based on a few different distributions that I would have to learn from using a neural network. We first had to implement a PyTorch simple neural network to solve the problem(s), and once we understood what the expected solutions should look similar to, we had to implement our solutions completely from scratch."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2024-05-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-05-01T00:00:00+00:00"><meta property="article:tag" content="Python"><meta property="article:tag" content="PyTorch"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Feed-forward Neural Network from Scratch"><meta name=twitter:description content="Note to reader
This was an assignment for my ML class at Northwestern, and while I do include the code in this, I want to be clear that this is my group&rsquo;s work and should not be used for any future assignments.
Overview
This assignment was a stepping stone in my interest in machine learning. We were given multiple datasets to work with based on a few different distributions that I would have to learn from using a neural network. We first had to implement a PyTorch simple neural network to solve the problem(s), and once we understood what the expected solutions should look similar to, we had to implement our solutions completely from scratch."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"http://localhost:1313/projects/"},{"@type":"ListItem","position":2,"name":"Feed-forward Neural Network from Scratch","item":"http://localhost:1313/projects/9_ffnn_scratch/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Feed-forward Neural Network from Scratch","name":"Feed-forward Neural Network from Scratch","description":"Note to reader This was an assignment for my ML class at Northwestern, and while I do include the code in this, I want to be clear that this is my group\u0026rsquo;s work and should not be used for any future assignments.\nOverview This assignment was a stepping stone in my interest in machine learning. We were given multiple datasets to work with based on a few different distributions that I would have to learn from using a neural network. We first had to implement a PyTorch simple neural network to solve the problem(s), and once we understood what the expected solutions should look similar to, we had to implement our solutions completely from scratch.\n","keywords":["Python","PyTorch"],"articleBody":"Note to reader This was an assignment for my ML class at Northwestern, and while I do include the code in this, I want to be clear that this is my group’s work and should not be used for any future assignments.\nOverview This assignment was a stepping stone in my interest in machine learning. We were given multiple datasets to work with based on a few different distributions that I would have to learn from using a neural network. We first had to implement a PyTorch simple neural network to solve the problem(s), and once we understood what the expected solutions should look similar to, we had to implement our solutions completely from scratch.\nIn doing so, I more clearly understood the math that was working within a neural network. It both debunked any fear on the subject and ignited a passion for the subject.\nThe dataset distributions that we needed to learn decision boundaries for were (also illustrated below):\nXOR Spiral Gaussian Center surround PyTorch Implementation Part one of this assignment was to utilize PyTorch to simply create a FFNN that could create the decision bounadies that are illustrated above.\nBecause this is an assignment that future students are likely to replicate, I will only include a few PyTorch snippets to get an idea of how this solution worked.\n# # Model architecture # class SimpleNN(torch.nn.Module): def __init__(self, input_size, hidden_size, output_size): super().__init__() self.f1 = torch.nn.Linear(input_size, hidden_size) self.relu = torch.nn.ReLU() self.f2 = torch.nn.Linear(hidden_size, output_size) def forward(self, x): x = self.f1(x) x = self.relu(x) x = self.f2(x) return x # # Training the model # def train(model, train_loader, valid_loader, num_epochs, criterion, optimizer): \"\"\" Parameters ---------- model: Pytorch nn.Module model train_loader: training set as a DataLoader() valid_loader: validation set as a DataLoader() num_epochs: Number of epochs criterion: Loss function optimizer: Optimization function \"\"\" train_losses = [] valid_losses = [] for epoch in range(num_epochs): model.train() running_train_loss = 0.0 for features, labels in train_loader: # Zero out gradients optimizer.zero_grad() # Predictions outputs = model(features) # Calculating the loss loss = criterion(outputs, labels) # Backprop loss.backward() # Updating parameters optimizer.step() # For analysis, we keep track of loss per batch running_train_loss += loss.item() epoch_train_loss = running_train_loss / len(train_loader) train_losses.append(epoch_train_loss) # # Validation set # model.eval() running_valid_loss = 0.0 with torch.no_grad(): for features, labels in valid_loader: outputs = model(features) loss = criterion(outputs, labels) running_valid_loss += loss.item() epoch_valid_loss = running_valid_loss / len(valid_loader) valid_losses.append(epoch_valid_loss) if (epoch + 1) % 50 == 0: print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Valid Loss: {epoch_valid_loss:.4f}\") return train_losses, valid_losses PyTorch Results The PyTorch results, after training, are found below and compare the difference between utilizing MSE and MCE for the task at hand. XOR Spiral Gaussian Center Surround FFNN from Scratch When implementing the NN from scratch, the first things that we had to accomplish was create the helper functions that were needed for calculating activations, as well as the derivatives of the functions.\nConceptual explaination For those who may be reading this and have 0 experience with neural networks, we need to use derivatives to properly calculate our backpropogation. The forward pass of the model (moving left to right) calculates a final prediction for what the solution should be given the data at hand. Once we have the prediction, we calculate the loss (how off the mark and/ or accurate we are compared to our expected output), and then we backpropogate to learn by how much to adjust the internal values of our neural network. So at first, our neural network contains weights and biases that are unlearned semi-random values, after each step, though, we update these parameters to help minimize the loss (minimize how many mistakes we make in our calculations), but taking the derivative of the loss function with respect to the paramaters within the model. All this means, is that we take the derivatives (calculus) to see by how much to adjust each of the weights within the network so that the model can perform better (hopefully) on the next iteration. I don’t expect this to have taught you neural networks, but I hope it was somewhat helpful to reframe it maybe a little bit differently for any newcomers.\nHelper functions Anyways, the helper functions that we use for these calculations exist here:\ndef z_score(df, scaler): \"\"\" Parameters ---------- df: pd.DataFrame with labels in the first column and features in following columns scaler: sklearn.preprocessing.StandardScaler fit to the training dataset feature values Returns ------- tuple: np.array of scaled features and np.array of labels \"\"\" features = df.iloc[:, 1:] labels = df.iloc[:, 0].values features = scaler.transform(features) indices_order = np.random.permutation(len(features)) features = features[indices_order] labels = labels[indices_order] return features, labels def label_encoder(labels): \"\"\" Parameters ---------- labels: np.array of training dataset labels Returns ------- dict: with one-hot encoded \"numerical encoding: label\" key: value pairs \"\"\" unique_labels = sorted(np.unique(labels)) labels_encoding = {i: l for i, l in enumerate(unique_labels)} return labels_encoding def encode_labels(labels, encoder): \"\"\" Parameters ---------- labels: np.array of training dataset labels encoder : dict with \"numerical encoding: label\" key: value pairs Returns ------- np.array of encoded labels \"\"\" return np.array([encoder[l] for l in labels]) def decode_labels(encoded_labels, encoder): \"\"\" Parameters ---------- encoded_labels: np.array of encoded labels encoder: dict with \"numerical encoding: label\" key: value pairs Returns ------- np.array: of decoded labels \"\"\" decoder = {value: key for key, value in encoder.items()} return np.array([decoder[l] for l in encoded_labels]) def encoded_labels_array(encoded_labels): \"\"\" Parameters ---------- encoded_labels: np.array of encoded labels Returns ------- np.array of encoded labels as arrays (e.g., encoded label 1 becomes [0, 1] and encoded label 0 becomes [1, 0]) \"\"\" num_unique_labels = len(np.unique(encoded_labels)) return np.array([np.array([1 if i == el else 0 for i in range(num_unique_labels)]) for el in encoded_labels]) def softmax(logits): \"\"\" Parameters ---------- logits: np.array of logits (raw output from output layer) Returns ------- np.array: of values after performing softmax (values sum to 1) \"\"\" exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True)) return exp_logits / np.sum(exp_logits, axis=1, keepdims=True) def mcce(softmax_logits, labels): \"\"\" Parameters ---------- softmax_logits: np.array of softmaxed logits (class probabilities) labels: np.array of true labels Returns ------- np.float64: Multi-class cross entropy loss \"\"\" return - np.sum(labels * np.log(softmax_logits)) / labels.shape[0] def sigmoid(unactivated): \"\"\" Parameters ---------- unactivated: np.array of unactivated input matrix (output matrix from last layer @ weights) Returns ------- np.array: Matrix after performing sigmoid operation on unactivated input matrix \"\"\" return 1 / (1 + np.exp(-unactivated)) def sigmoid_derivative(activated): \"\"\" Parameters ---------- activated: np.array of activated input matrix Returns ------- np.array: Matrix after performing sigmoid derivative operation on activated input matrix \"\"\" return sigmoid(activated) * (1 - sigmoid(activated)) def relu(unactivated): \"\"\" Parameters ---------- unactivated: np.array of unactivated input matrix (output matrix from last layer @ weights) Returns ------- np.array: Matrix after performing relu operation on unactivated input matrix \"\"\" return np.maximum(0, unactivated) def relu_derivative(activated): \"\"\" Parameters ---------- activated : np.array of activated input matrix Returns ------- np.array: Matrix after performing relu derivative operation on activated input matrix \"\"\" return (activated \u003e 0).astype(float) Forward pass We decided that it would be best to create a model architecture that is similar to that of the PyTorch implementation. Therefore, our forward pass consists of a fully connected layer between the inputs and the first hidden layer, then we add the bias term, then we use the relu activation function to get the output of the first hidden layer.\nNext we have another fully connected layer between the results from the first hidden layer and the weights of the second hidden layer, again add the bias terms, and then take the softmax activation function. The softmax function is used here to create a probability distribution of our predictions, and we will then use this vector of percentages that add up to 1, to take our prediction.\nThe forward pass (the shapes between the input to first hidden layer and the hidden layer to the output layer are not shown, but exist in shape_i_h, shape_h_o):\ndef forward(self, features, train=False): \"\"\" Parameters ---------- features: np.array of examples with each example being an array of feature values train : Optional[bool] - True if training the network (i.e., if intermediate results are needed to compute gradients), by default False Returns ------- np.array: of outputs for each example with each output being an array of class probabilities \"\"\" forward_results = {} forward_results['features'] = features i_h = features @ self.w_i_h forward_results['i_h'] = i_h i_h_b = i_h + self.b_i_h forward_results['i_h_b'] = i_h_b a_i_h_b = relu(i_h_b) forward_results['a_i_h_b'] = a_i_h_b h_o = a_i_h_b @ self.w_h_o forward_results['h_o'] = h_o h_o_b = h_o + self.b_h_o forward_results['h_o_b'] = h_o_b a_h_o_b = softmax(h_o_b) forward_results['a_h_o_b'] = a_h_o_b if train: return a_h_o_b, forward_results return a_h_o_b Backward pass This is where we have to calculate the derivative of the loss function with respect to the model paramters to learn by how much to adjust the model parameters to minimize the loss function. Also, for learning purposes, the derivative of the loss function with respect to the model paramters will calculate by how much to adjust the values to maximize the loss function we negate this to minimize it.\nAll calculations were done by hand outside of the code, and then later implemented based on our calculations.\ndef backward(self, forward_results, labels): \"\"\" Parameters ---------- forward_results: dict of all results (input, intermediates, and output) from forward a pass labels: np.array of encoded labels Returns ------- dict: of gradient matrices of weight and bias matrices \"\"\" gradients = {} probs = forward_results['a_h_o_b'] dL_dh_o_b = probs - labels a_i_h_b = forward_results['a_i_h_b'] gradients['w_h_o'] = a_i_h_b.T @ dL_dh_o_b gradients['b_h_o'] = np.sum(dL_dh_o_b, axis=0, keepdims=True) dL_da_i_h_b = dL_dh_o_b @ self.w_h_o.T dL_di_h_b = dL_da_i_h_b * relu_derivative(forward_results['a_i_h_b']) gradients['w_i_h'] = forward_results['features'].T @ dL_di_h_b gradients['b_i_h'] = np.sum(dL_di_h_b, axis=0, keepdims=True) return gradients Updating the parameters Now that we have backpropogated, we have a way to adjust the actual weights and biases within the model.\ndef update_weights(self, gradients, learning_rate): \"\"\" Parameters ---------- gradients: dict of gradient matrices of weight and bias matrices learning_rate: float that controls the size of the step taken during the update \"\"\" self.w_h_o -= (learning_rate * gradients['w_h_o']) self.b_h_o -= (learning_rate * gradients['b_h_o']) self.w_i_h -= (learning_rate * gradients['w_i_h']) self.b_i_h -= (learning_rate * gradients['b_i_h']) Training the model Training the model works extremely similarly to the PyTorch implementation, it just uses our own helper functions that I have included above to go throught he forward pass, backward pass, and then how we should update our weights for the next iteration.\nFFNN from Scratch Results Spiral ","wordCount":"1709","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-05-01T00:00:00Z","dateModified":"2024-05-01T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/projects/9_ffnn_scratch/"},"publisher":{"@type":"Organization","name":"ExampleSite","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li><li><a href=http://localhost:1313/projects/ title=projects><span>projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/projects/>Projects</a></div><h1 class="post-title entry-hint-parent">Feed-forward Neural Network from Scratch</h1></header><div class=post-content><h2 id=note-to-reader>Note to reader<a hidden class=anchor aria-hidden=true href=#note-to-reader>#</a></h2><p><em>This was an assignment for my ML class at Northwestern, and while I do include the code in this, I want to be clear that this is my group&rsquo;s work and should not be used for any future assignments.</em></p><h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><p>This assignment was a stepping stone in my interest in machine learning. We were given multiple datasets to work with based on a few different distributions that I would have to learn from using a neural network. We first had to implement a PyTorch simple neural network to solve the problem(s), and once we understood what the expected solutions should look similar to, we had to implement our solutions completely from scratch.</p><p>In doing so, I more clearly understood the math that was working within a neural network. It both debunked any fear on the subject and ignited a passion for the subject.</p><p>The dataset distributions that we needed to learn decision boundaries for were (also illustrated below):</p><ol><li>XOR</li><li>Spiral</li><li>Gaussian</li><li>Center surround</li></ol><h2 id=pytorch-implementation>PyTorch Implementation<a hidden class=anchor aria-hidden=true href=#pytorch-implementation>#</a></h2><p>Part one of this assignment was to utilize PyTorch to simply create a FFNN that could create the decision bounadies that are illustrated above.</p><p>Because this is an assignment that future students are likely to replicate, I will only include a few PyTorch snippets to get an idea of how this solution worked.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1># Model architecture</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SimpleNN</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>f1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>f2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>output_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>f1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>f2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1># Training the model</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>train_loader</span><span class=p>,</span> <span class=n>valid_loader</span><span class=p>,</span> <span class=n>num_epochs</span><span class=p>,</span> <span class=n>criterion</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters
</span></span></span><span class=line><span class=cl><span class=s2>    ----------
</span></span></span><span class=line><span class=cl><span class=s2>    model: Pytorch nn.Module model
</span></span></span><span class=line><span class=cl><span class=s2>    train_loader: training set as a DataLoader()
</span></span></span><span class=line><span class=cl><span class=s2>    valid_loader: validation set as a DataLoader()
</span></span></span><span class=line><span class=cl><span class=s2>    num_epochs: Number of epochs
</span></span></span><span class=line><span class=cl><span class=s2>    criterion: Loss function
</span></span></span><span class=line><span class=cl><span class=s2>    optimizer: Optimization function
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>train_losses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>valid_losses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>running_train_loss</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>features</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># Zero out gradients</span>
</span></span><span class=line><span class=cl>            <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Predictions</span>
</span></span><span class=line><span class=cl>            <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Calculating the loss</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Backprop</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Updating parameters</span>
</span></span><span class=line><span class=cl>            <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># For analysis, we keep track of loss per batch</span>
</span></span><span class=line><span class=cl>            <span class=n>running_train_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>epoch_train_loss</span> <span class=o>=</span> <span class=n>running_train_loss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>train_loader</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>train_losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>epoch_train_loss</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1>#</span>
</span></span><span class=line><span class=cl>        <span class=c1># Validation set</span>
</span></span><span class=line><span class=cl>        <span class=c1>#</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>running_valid_loss</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>features</span><span class=p>,</span> <span class=n>labels</span> <span class=ow>in</span> <span class=n>valid_loader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                
</span></span><span class=line><span class=cl>                <span class=n>running_valid_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>        <span class=n>epoch_valid_loss</span> <span class=o>=</span> <span class=n>running_valid_loss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>valid_loader</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>valid_losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>epoch_valid_loss</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>epoch</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>%</span> <span class=mi>50</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=n>num_epochs</span><span class=si>}</span><span class=s2>, Train Loss: </span><span class=si>{</span><span class=n>epoch_train_loss</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, Valid Loss: </span><span class=si>{</span><span class=n>epoch_valid_loss</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>train_losses</span><span class=p>,</span> <span class=n>valid_losses</span>
</span></span></code></pre></div><h4 id=pytorch-results>PyTorch Results<a hidden class=anchor aria-hidden=true href=#pytorch-results>#</a></h4><ul><li>The PyTorch results, after training, are found below and compare the difference between utilizing MSE and MCE for the task at hand.</li></ul><h4 id=xor>XOR<a hidden class=anchor aria-hidden=true href=#xor>#</a></h4><div><img src=/images/ml_hw3/pytorch_xor.png alt="PyTorch implementation of XOR" width=300px style="display:block;margin:0 auto"></div><h4 id=spiral>Spiral<a hidden class=anchor aria-hidden=true href=#spiral>#</a></h4><div><img src=/images/ml_hw3/pytorch_spiral.png alt="PyTorch implementation of spiral" width=300px style="display:block;margin:0 auto"></div><h4 id=gaussian>Gaussian<a hidden class=anchor aria-hidden=true href=#gaussian>#</a></h4><div><img src=/images/ml_hw3/pytorch_gaussian.png alt="PyTorch implementation of gaussian" width=300px style="display:block;margin:0 auto"></div><h4 id=center-surround>Center Surround<a hidden class=anchor aria-hidden=true href=#center-surround>#</a></h4><div><img src=/images/ml_hw3/pytorch_center_surround.png alt="PyTorch implementation of center surround" width=300px style="display:block;margin:0 auto"></div><h2 id=ffnn-from-scratch>FFNN from Scratch<a hidden class=anchor aria-hidden=true href=#ffnn-from-scratch>#</a></h2><p>When implementing the NN from scratch, the first things that we had to accomplish was create the helper functions that were needed for calculating activations, as well as the derivatives of the functions.</p><h4 id=conceptual-explaination>Conceptual explaination<a hidden class=anchor aria-hidden=true href=#conceptual-explaination>#</a></h4><hr><p>For those who may be reading this and have 0 experience with neural networks, we need to use derivatives to properly calculate our backpropogation. The forward pass of the model (moving left to right) calculates a final prediction for what the solution should be given the data at hand. Once we have the prediction, we calculate the loss (how off the mark and/ or accurate we are compared to our expected output), and then we backpropogate to learn by how much to adjust the internal values of our neural network. So at first, our neural network contains weights and biases that are unlearned semi-random values, after each step, though, we update these parameters to help <em>minimize the loss</em> (minimize how many mistakes we make in our calculations), but taking the derivative of the loss function with respect to the paramaters within the model. All this means, is that we take the derivatives (calculus) to see by how much to adjust each of the weights within the network so that the model can perform better (hopefully) on the next iteration. I don&rsquo;t expect this to have taught you neural networks, but I hope it was somewhat helpful to reframe it maybe a little bit differently for any newcomers.</p><hr><h4 id=helper-functions>Helper functions<a hidden class=anchor aria-hidden=true href=#helper-functions>#</a></h4><p>Anyways, the helper functions that we use for these calculations exist here:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>z_score</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>scaler</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters
</span></span></span><span class=line><span class=cl><span class=s2>    ----------
</span></span></span><span class=line><span class=cl><span class=s2>    df: pd.DataFrame with labels in the first column and features in following columns
</span></span></span><span class=line><span class=cl><span class=s2>    scaler: sklearn.preprocessing.StandardScaler fit to the training dataset feature values
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns
</span></span></span><span class=line><span class=cl><span class=s2>    -------
</span></span></span><span class=line><span class=cl><span class=s2>    tuple: np.array of scaled features and np.array of labels
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>features</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>    <span class=n>labels</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>values</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>features</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>indices_order</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>permutation</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>features</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>features</span> <span class=o>=</span> <span class=n>features</span><span class=p>[</span><span class=n>indices_order</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>labels</span> <span class=o>=</span> <span class=n>labels</span><span class=p>[</span><span class=n>indices_order</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>features</span><span class=p>,</span> <span class=n>labels</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>label_encoder</span><span class=p>(</span><span class=n>labels</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters
</span></span></span><span class=line><span class=cl><span class=s2>    ----------
</span></span></span><span class=line><span class=cl><span class=s2>    labels: np.array of training dataset labels
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns
</span></span></span><span class=line><span class=cl><span class=s2>    -------
</span></span></span><span class=line><span class=cl><span class=s2>    dict: with one-hot encoded &#34;numerical encoding: label&#34; key: value pairs
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>unique_labels</span> <span class=o>=</span> <span class=nb>sorted</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>labels</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>labels_encoding</span> <span class=o>=</span> <span class=p>{</span><span class=n>i</span><span class=p>:</span> <span class=n>l</span> <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>l</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>unique_labels</span><span class=p>)}</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>labels_encoding</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>encode_labels</span><span class=p>(</span><span class=n>labels</span><span class=p>,</span> <span class=n>encoder</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters
</span></span></span><span class=line><span class=cl><span class=s2>    ----------
</span></span></span><span class=line><span class=cl><span class=s2>    labels: np.array of training dataset labels
</span></span></span><span class=line><span class=cl><span class=s2>    encoder : dict with &#34;numerical encoding: label&#34; key: value pairs
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns
</span></span></span><span class=line><span class=cl><span class=s2>    -------
</span></span></span><span class=line><span class=cl><span class=s2>    np.array of encoded labels
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>encoder</span><span class=p>[</span><span class=n>l</span><span class=p>]</span> <span class=k>for</span> <span class=n>l</span> <span class=ow>in</span> <span class=n>labels</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>decode_labels</span><span class=p>(</span><span class=n>encoded_labels</span><span class=p>,</span> <span class=n>encoder</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters
</span></span></span><span class=line><span class=cl><span class=s2>    ----------
</span></span></span><span class=line><span class=cl><span class=s2>    encoded_labels: np.array of encoded labels
</span></span></span><span class=line><span class=cl><span class=s2>    encoder: dict with &#34;numerical encoding: label&#34; key: value pairs
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns
</span></span></span><span class=line><span class=cl><span class=s2>    -------
</span></span></span><span class=line><span class=cl><span class=s2>    np.array: of decoded labels
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>decoder</span> <span class=o>=</span> <span class=p>{</span><span class=n>value</span><span class=p>:</span> <span class=n>key</span> <span class=k>for</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span> <span class=ow>in</span> <span class=n>encoder</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>decoder</span><span class=p>[</span><span class=n>l</span><span class=p>]</span> <span class=k>for</span> <span class=n>l</span> <span class=ow>in</span> <span class=n>encoded_labels</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>encoded_labels_array</span><span class=p>(</span><span class=n>encoded_labels</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters
</span></span></span><span class=line><span class=cl><span class=s2>    ----------
</span></span></span><span class=line><span class=cl><span class=s2>    encoded_labels: np.array of encoded labels
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns
</span></span></span><span class=line><span class=cl><span class=s2>    -------
</span></span></span><span class=line><span class=cl><span class=s2>    np.array of encoded labels as arrays (e.g., encoded label 1 becomes [0, 1] and encoded label 0 becomes [1, 0])
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>num_unique_labels</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>unique</span><span class=p>(</span><span class=n>encoded_labels</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span> <span class=k>if</span> <span class=n>i</span> <span class=o>==</span> <span class=n>el</span> <span class=k>else</span> <span class=mi>0</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_unique_labels</span><span class=p>)])</span> <span class=k>for</span> <span class=n>el</span> <span class=ow>in</span> <span class=n>encoded_labels</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters
</span></span></span><span class=line><span class=cl><span class=s2>    ----------
</span></span></span><span class=line><span class=cl><span class=s2>    logits: np.array of logits (raw output from output layer)
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns
</span></span></span><span class=line><span class=cl><span class=s2>    -------
</span></span></span><span class=line><span class=cl><span class=s2>    np.array: of values after performing softmax (values sum to 1)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>exp_logits</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>logits</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>  
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>exp_logits</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>exp_logits</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>mcce</span><span class=p>(</span><span class=n>softmax_logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters
</span></span></span><span class=line><span class=cl><span class=s2>    ----------
</span></span></span><span class=line><span class=cl><span class=s2>    softmax_logits: np.array of softmaxed logits (class probabilities)
</span></span></span><span class=line><span class=cl><span class=s2>    labels: np.array of true labels 
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns
</span></span></span><span class=line><span class=cl><span class=s2>    -------
</span></span></span><span class=line><span class=cl><span class=s2>    np.float64: Multi-class cross entropy loss
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>labels</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>softmax_logits</span><span class=p>))</span> <span class=o>/</span> <span class=n>labels</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>sigmoid</span><span class=p>(</span><span class=n>unactivated</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters
</span></span></span><span class=line><span class=cl><span class=s2>    ----------
</span></span></span><span class=line><span class=cl><span class=s2>    unactivated: np.array of unactivated input matrix (output matrix from last layer @ weights)
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns
</span></span></span><span class=line><span class=cl><span class=s2>    -------
</span></span></span><span class=line><span class=cl><span class=s2>    np.array: Matrix after performing sigmoid operation on unactivated input matrix
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>unactivated</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>sigmoid_derivative</span><span class=p>(</span><span class=n>activated</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters
</span></span></span><span class=line><span class=cl><span class=s2>    ----------
</span></span></span><span class=line><span class=cl><span class=s2>    activated: np.array of activated input matrix
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns
</span></span></span><span class=line><span class=cl><span class=s2>    -------
</span></span></span><span class=line><span class=cl><span class=s2>    np.array: Matrix after performing sigmoid derivative operation on activated input matrix
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>activated</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>activated</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>relu</span><span class=p>(</span><span class=n>unactivated</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters
</span></span></span><span class=line><span class=cl><span class=s2>    ----------
</span></span></span><span class=line><span class=cl><span class=s2>    unactivated: np.array of unactivated input matrix (output matrix from last layer @ weights)
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns
</span></span></span><span class=line><span class=cl><span class=s2>    -------
</span></span></span><span class=line><span class=cl><span class=s2>    np.array: Matrix after performing relu operation on unactivated input matrix
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>unactivated</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>relu_derivative</span><span class=p>(</span><span class=n>activated</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters
</span></span></span><span class=line><span class=cl><span class=s2>    ----------
</span></span></span><span class=line><span class=cl><span class=s2>    activated : np.array of activated input matrix
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns
</span></span></span><span class=line><span class=cl><span class=s2>    -------
</span></span></span><span class=line><span class=cl><span class=s2>    np.array: Matrix after performing relu derivative operation on activated input matrix
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=n>activated</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=nb>float</span><span class=p>)</span>
</span></span></code></pre></div><h4 id=forward-pass>Forward pass<a hidden class=anchor aria-hidden=true href=#forward-pass>#</a></h4><p>We decided that it would be best to create a model architecture that is similar to that of the PyTorch implementation. Therefore, our forward pass consists of a fully connected layer between the inputs and the first hidden layer, then we add the bias term, then we use the relu activation function to get the output of the first hidden layer.</p><p>Next we have another fully connected layer between the results from the first hidden layer and the weights of the second hidden layer, again add the bias terms, and then take the softmax activation function. The softmax function is used here to create a probability distribution of our predictions, and we will then use this vector of percentages that add up to 1, to take our prediction.</p><p>The forward pass (the shapes between the input to first hidden layer and the hidden layer to the output layer are not shown, but exist in shape_i_h, shape_h_o):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>features</span><span class=p>,</span> <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters
</span></span></span><span class=line><span class=cl><span class=s2>    ----------
</span></span></span><span class=line><span class=cl><span class=s2>    features: np.array of examples with each example being an array of feature values
</span></span></span><span class=line><span class=cl><span class=s2>    train : Optional[bool] - True if training the network (i.e., if intermediate results are needed to compute gradients), by default False
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns
</span></span></span><span class=line><span class=cl><span class=s2>    -------
</span></span></span><span class=line><span class=cl><span class=s2>    np.array: of outputs for each example with each output being an array of class probabilities
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>forward_results</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>forward_results</span><span class=p>[</span><span class=s1>&#39;features&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>features</span>
</span></span><span class=line><span class=cl>    <span class=n>i_h</span> <span class=o>=</span> <span class=n>features</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>w_i_h</span>
</span></span><span class=line><span class=cl>    <span class=n>forward_results</span><span class=p>[</span><span class=s1>&#39;i_h&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>i_h</span>
</span></span><span class=line><span class=cl>    <span class=n>i_h_b</span> <span class=o>=</span> <span class=n>i_h</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>b_i_h</span>
</span></span><span class=line><span class=cl>    <span class=n>forward_results</span><span class=p>[</span><span class=s1>&#39;i_h_b&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>i_h_b</span>
</span></span><span class=line><span class=cl>    <span class=n>a_i_h_b</span> <span class=o>=</span> <span class=n>relu</span><span class=p>(</span><span class=n>i_h_b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>forward_results</span><span class=p>[</span><span class=s1>&#39;a_i_h_b&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>a_i_h_b</span>        
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>h_o</span> <span class=o>=</span> <span class=n>a_i_h_b</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>w_h_o</span>
</span></span><span class=line><span class=cl>    <span class=n>forward_results</span><span class=p>[</span><span class=s1>&#39;h_o&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>h_o</span>        
</span></span><span class=line><span class=cl>    <span class=n>h_o_b</span> <span class=o>=</span> <span class=n>h_o</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>b_h_o</span>
</span></span><span class=line><span class=cl>    <span class=n>forward_results</span><span class=p>[</span><span class=s1>&#39;h_o_b&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>h_o_b</span>
</span></span><span class=line><span class=cl>    <span class=n>a_h_o_b</span> <span class=o>=</span> <span class=n>softmax</span><span class=p>(</span><span class=n>h_o_b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>forward_results</span><span class=p>[</span><span class=s1>&#39;a_h_o_b&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>a_h_o_b</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>train</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>a_h_o_b</span><span class=p>,</span> <span class=n>forward_results</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>a_h_o_b</span>
</span></span></code></pre></div><h4 id=backward-pass>Backward pass<a hidden class=anchor aria-hidden=true href=#backward-pass>#</a></h4><p>This is where we have to calculate the derivative of the loss function with respect to the model paramters to learn by how much to adjust the model parameters to minimize the loss function. Also, for learning purposes, the derivative of the loss function with respect to the model paramters will calculate by how much to adjust the values to maximize the loss function we negate this to minimize it.</p><p>All calculations were done by hand outside of the code, and then later implemented based on our calculations.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>backward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>forward_results</span><span class=p>,</span> <span class=n>labels</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters
</span></span></span><span class=line><span class=cl><span class=s2>    ----------
</span></span></span><span class=line><span class=cl><span class=s2>    forward_results: dict of all results (input, intermediates, and output) from forward a pass
</span></span></span><span class=line><span class=cl><span class=s2>    labels: np.array of encoded labels
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns
</span></span></span><span class=line><span class=cl><span class=s2>    -------
</span></span></span><span class=line><span class=cl><span class=s2>    dict: of gradient matrices of weight and bias matrices
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>gradients</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>probs</span> <span class=o>=</span> <span class=n>forward_results</span><span class=p>[</span><span class=s1>&#39;a_h_o_b&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>dL_dh_o_b</span> <span class=o>=</span> <span class=n>probs</span> <span class=o>-</span> <span class=n>labels</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>a_i_h_b</span> <span class=o>=</span> <span class=n>forward_results</span><span class=p>[</span><span class=s1>&#39;a_i_h_b&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>gradients</span><span class=p>[</span><span class=s1>&#39;w_h_o&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>a_i_h_b</span><span class=o>.</span><span class=n>T</span> <span class=o>@</span> <span class=n>dL_dh_o_b</span>
</span></span><span class=line><span class=cl>    <span class=n>gradients</span><span class=p>[</span><span class=s1>&#39;b_h_o&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dL_dh_o_b</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>dL_da_i_h_b</span> <span class=o>=</span> <span class=n>dL_dh_o_b</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>w_h_o</span><span class=o>.</span><span class=n>T</span> 
</span></span><span class=line><span class=cl>    <span class=n>dL_di_h_b</span> <span class=o>=</span> <span class=n>dL_da_i_h_b</span> <span class=o>*</span> <span class=n>relu_derivative</span><span class=p>(</span><span class=n>forward_results</span><span class=p>[</span><span class=s1>&#39;a_i_h_b&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>gradients</span><span class=p>[</span><span class=s1>&#39;w_i_h&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>forward_results</span><span class=p>[</span><span class=s1>&#39;features&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>T</span> <span class=o>@</span> <span class=n>dL_di_h_b</span> 
</span></span><span class=line><span class=cl>    <span class=n>gradients</span><span class=p>[</span><span class=s1>&#39;b_i_h&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dL_di_h_b</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>gradients</span>
</span></span></code></pre></div><h4 id=updating-the-parameters>Updating the parameters<a hidden class=anchor aria-hidden=true href=#updating-the-parameters>#</a></h4><p>Now that we have backpropogated, we have a way to adjust the actual weights and biases within the model.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>update_weights</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>gradients</span><span class=p>,</span> <span class=n>learning_rate</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters
</span></span></span><span class=line><span class=cl><span class=s2>    ----------
</span></span></span><span class=line><span class=cl><span class=s2>    gradients: dict of gradient matrices of weight and bias matrices
</span></span></span><span class=line><span class=cl><span class=s2>    learning_rate: float that controls the size of the step taken during the update
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>w_h_o</span> <span class=o>-=</span> <span class=p>(</span><span class=n>learning_rate</span> <span class=o>*</span> <span class=n>gradients</span><span class=p>[</span><span class=s1>&#39;w_h_o&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>b_h_o</span> <span class=o>-=</span> <span class=p>(</span><span class=n>learning_rate</span> <span class=o>*</span> <span class=n>gradients</span><span class=p>[</span><span class=s1>&#39;b_h_o&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>w_i_h</span> <span class=o>-=</span> <span class=p>(</span><span class=n>learning_rate</span> <span class=o>*</span> <span class=n>gradients</span><span class=p>[</span><span class=s1>&#39;w_i_h&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>b_i_h</span> <span class=o>-=</span> <span class=p>(</span><span class=n>learning_rate</span> <span class=o>*</span> <span class=n>gradients</span><span class=p>[</span><span class=s1>&#39;b_i_h&#39;</span><span class=p>])</span>
</span></span></code></pre></div><h4 id=training-the-model>Training the model<a hidden class=anchor aria-hidden=true href=#training-the-model>#</a></h4><p>Training the model works extremely similarly to the PyTorch implementation, it just uses our own helper functions that I have included above to go throught he forward pass, backward pass, and then how we should update our weights for the next iteration.</p><h4 id=ffnn-from-scratch-results>FFNN from Scratch Results<a hidden class=anchor aria-hidden=true href=#ffnn-from-scratch-results>#</a></h4><h4 id=spiral-1>Spiral<a hidden class=anchor aria-hidden=true href=#spiral-1>#</a></h4><div><img src=/images/ml_hw3/scratch_spiral.png alt="PyTorch implementation of spiral
        width=" 300px" style="display:block;margin:0 auto"></div><h4 id=xor-1>XOR<a hidden class=anchor aria-hidden=true href=#xor-1>#</a></h4><div><img src=/images/ml_hw3/scratch_xor.png alt="PyTorch implementation of xor" width=300px style="display:block;margin:0 auto"></div><h4 id=gaussian-1>Gaussian<a hidden class=anchor aria-hidden=true href=#gaussian-1>#</a></h4><div><img src=/images/ml_hw3/scratch_gaussian.png alt="PyTorch implementation of gaussian" width=300px style="display:block;margin:0 auto"></div><h4 id=center-surround-1>Center Surround<a hidden class=anchor aria-hidden=true href=#center-surround-1>#</a></h4><div><img src=/images/ml_hw3/scratch_center_surround.png alt="PyTorch implementation of center surround" width=300px style="display:block;margin:0 auto"></div><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>This was an extremely fulfilling project because it taught me to be less afraid of the equations that exist within ML. And also, that the equations that we use all make very clear sense and can explain everything within the space. I apologize to anyone who is less familiar in the ML space, this page is not meant for beginners, it is only a summary for anyone interested in my work. I may in the future create a site on my portfolio that goes through all the steps needed to understand what is happening from scratch so that others may implement it on their own for fun!</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/python/>Python</a></li><li><a href=http://localhost:1313/tags/pytorch/>PyTorch</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>ExampleSite</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>